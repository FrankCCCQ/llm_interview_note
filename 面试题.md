todo：投机采样、medusa heads、continuous batching


## 1. 大模型推理优化
归纳为6类：分布式优化、低比特优化、算子优化、访存优化、服务并发优化、其他
分布式优化：通信层面的优化（TP、DP、PP + nccl通信优化）
低比特优化：降低显存占用（int8、int4权重量化｜AWQ自适应量化、KV cache）
算子优化：提高cuda算子的计算效率（算子融合和GEMM高斯该性能算子）
访存优化：减少gpu对hbm的读写（flashattention）
服务并发优化：continues batching、dynamic batching、async serveing
其他：投机采样、medusa heads、lookahead decoding、eagle

## 2. 大模型的吞吐太小怎么办？
吞吐率 = 处理请求数量 / 处理时间

处理请求数量： batch_size，node数量
处理时间 ： 模型forward的时间

### 1. 模型单次推理的时间 + 模型能支持的并发数量 -> 降低模型的推理延时+提供并发数量
如何降低推理延时：权重+激活量化，比如GPU利用率有空余，可以采用投机采样（小模型猜测+大模型验证）；访存优化：flashattention
如何提高模型的并发数：1. batch_size增加（静态batch、 动态batch、 continuous batching）；业务允许的情况下做水平扩展，增加资源数量，负载均衡

### 2. 结合业务
1. 降低推理延时：优化单个请求的体验；提高并行度，优化的多个请求的处理时间，但对每个请求可能会导致10%-30%左右的延时增加
continuous batching：LLM场景下不能假定请求的输入序列和输出序列的长度，所以对于静态batching和动态batching来说，输入输出的变化可能会严重GPU的利用率降低，所以使用**continuous batching迭代调度**（当一个序列出现一个结束生成的标记，就可以在这个序列后面插入新的序列）
服务上也需要根据是不是流式输出来进行优化。结合异步进行解码


## 如何解决badcase
1. 前置模块 + 后处理 + 调整prompt + 模型微调
*前置模块*：对于一些高频case，直接在前置模块进行返回，配置一些简单规则实现
*后处理*：比如LLM幻觉，可以后处理模块在进行2次过滤，
*修改prompt*：不要求时效性前提下，比如对输出的关键词有要求、对人设遵循能力、一致性等
*模型微调*：成本比较高、微调后对原来效果可能会出现影响。所以一般累计case，固定周期进行模型更新


## DeepSeek MoE的创新与改进

### Mixtral的MoE结构（对比基线）
- 将Attention层后的MLP模块替换为MoE模块（Router + 多个Dense Expert）
- 使用8个7B参数量的Dense Expert，每次仅激活2个
- 工作流程：Token经Router计算→选择Top-k Expert激活→加权组合结果

### DeepSeek的MoE结构创新
- **组成部分**：
  - Router模块：负责分发Token到不同Expert
  - Routed Experts：256个专家，每次激活8个
  - Shared Expert：1个通用专家，始终激活
- **实现细节**：
  - Routed Experts分组管理，先计算每组内Top-2专家的相似度得分和
  - 选择得分最高的Top-k组，再从每组中选择Top-k专家
  - 最终输出 = Shared Expert输出 + Routed Experts加权输出

### DeepSeek MoE结构的优势
1. **知识分离与优化**
   - 解决传统MoE中Expert间存在的冗余问题
   - Shared Expert处理通用知识，Routed Experts专注差异化知识
   - 减轻了单个Routed Expert需要学习的知识量

2. **计算效率提升**
   - 背景：MoE训练中Expert分布在不同设备上，产生大量通信开销
   - 优化方法：在通信流处于工作状态时，同步使用计算流进行后续计算
   - 效果：Shared Expert和Routed Experts的通信相互独立，可以隐藏通信时间

  
## RAG概述
### rag解释
1. rag是一种结合LLM生成能力和外部知识库的检索能力的技术，来解决LLM潜在的一些问题：知识截断（LLM的知识仅限于训练数据的截止时间）、幻觉（LLM生成的不准确或者虚假的信息）、缺乏特定领域的私有知识（LLM不能访问训练数据之外的（实时的、私有的信息））
### rag的整体流程
#### 1.离线处理事，知识库的构建和向量化
- 知识文档库：构建包含各类文档、知识和信息的库
- 文档向量化：通过embedding引擎将文本内容转换为高维向量，捕捉文本的语义信息
- 向量数据库：存储所有文档的向量表示，建立高效的检索索引，为后续的实时查询做准备（ES）

#### 2.在线查询处理，用户实时查询并生成回答
- 用户查询：接受到用户输入的query
- 查询向量化：转成与文档相同的向量化表示（检索+重排，检索用余弦相似度，重排用bert就可以）
- 语义检索：在向量库中查询与语义最相似的知识
- LLM：将用户query与查询的结果结合，增强上下文
- 生成回答

## 微调效果评估
1. 评估集
2. 评估指标：依据任务而定：人工评估（回复准确性、相关性、指令遵循、安全，AB test）+自动化指标（eda，ebs用召回；机器翻译用BLEU来衡量n-gram精度） + 文本生成，风格模仿（人工+困惑度）+代码生成（codeBLEU+单元测试）
3. 设立baseline：对比微调前的模型，不同微调方法对比
4. 定性分析：查看实际的输出，分析badcase
5. 维护golden集合，看gsb（GSB = Grammaticality语法, Semantics语义, Brevity简洁性）

## model.eval()的影响
### dropout 
是训练的时候将某层的神经元输出以p（如10%）的概率随机置为零，以减缓过拟合；但是测试的时候不会进行dropout；
**为训练和推理的行为一致，会在训练时候将输出 scaled by a factor of $\frac{1}{(1-p)}$**

### batchnorm
利用优化器的momentum进行


## 大模型幻觉怎么处理
《Survey of Hallucination in Natural Language Generation》
幻觉定义为：内在幻觉（生成内容与源数据相互矛盾）；外在幻觉（生成内容无法从源数据中进行验证，可能正确或者错误）
**幻觉的原因**
1. 数据驱动：训练数据源和当前参考不匹配导致的幻觉问题，如数据对不对齐，导致生成不忠实的文本
2. 表示和解码的不完善：编码器理解能力的缺陷和解码器策略错误可能导致模型幻觉，解码器可能关注错误的输入部分，或使用增加幻觉风险的解码策略（基于采样解码中的随机性）
3. 参数知识偏见：预训练模型可能偏好参数中的知识，而不是输入。

**幻觉怎么解决**
1. 构建忠实数据集：创建含准确事目标的数据集，可以应对特定任务，耗费资源
2. 自动清洗数据：在实例级别过滤幻觉内容，或针对参数修正输入数据，*适用于结构化数据到文本任务*
3. 信息增强：用外部信息增强到输入数据、更好的对齐目标、提高模型的语义理解能力，减少与源的偏离

**实际业务**
针对一些垂类任务，如果需要的领域知识并不知道预训练的数据域中，且难以通过继续训练将特定知识引入到模型中，**可以引导模型有选择的进行回答**

## 大模型涌现现象出现的原因（其实没有一个参考答案）
模型涌现（Emergence）是指大型语言模型在规模（参数量、数据量、计算量）增大到一定程度后，突然表现出一些在小规模模型上不具备或表现不佳的复杂能力（如算术推理、代码生成、思维链推理等），这些能力似乎不是通过模型规模的线性外推就能预测的。
### 1.规模是关键因素
- 参数量：更大的参数量说明模型有更大的容量学习和存储世界知识、复杂模式和抽象概念
- 数据量：海量的数据识得模型能够接触到足够多的实例、从而学习到细粒度的语言规律、推理模式和各种技能
- 计算量：足够的计算量可以支承更大规模的模型在海量数据上进行充分训练
### 2.任务的复杂性和能力阈值：复杂任务可能需要多种基础能力的组合，当模型的规模足够大，能够同时掌握必要的基础技能，并学会如何组合这些能力，才能在复杂任务上展现出显著的性能。可能像是一种threshold，低于这个值，即便模型能力提升，也难以展现出涌现的效果
### 3.训练目标和隐式学习：简单的NTP，在大规模数据和模型上训练的时候，可能隐式地迫使模型区学习到语义、语法、常识、甚至推理模式。这些都是为了更加准确的预测下一个token。
### 4.评估指标的非线性：从评估上也可能造成涌现，当模型能力在某一个任务上超过一个threshold的时候，可能得分就会大幅度提高
### 5.组合泛化：大模型可能更加擅长将学习到的基本概念或者技能进行组合、来解决新的、更加复杂的问题，这种组合能力可能随着规模的增长而增加
### 6.相变：物理上的概念，也是说模型能力超过某一个threshold之后，其参数结构或者学习动态发生质变，导致新能力出现

## LLM复读机，口吃
复读机是指大模型在生成文本时，倾向于重复之前已经生成过的词语、短语和句子、甚至整段
### 原因
- 训练数据偏差：训练数据中可能存在大量的重复模式或者样板文字，导致模型学习到啦这种重复倾向
- 解码策略：1.greedy search、beam search：这种确定性或者半确定性的解码策略倾向于使用当前最高prob的词，如果某个词或者短句刚出现过，它在目前的上下文中的prob可能仍然很高，导致被反复选中； 2. low temperature：低温导致采样分布更加尖锐，模型倾向于选择更大prob的token，也会增加重复性
- 模型本身问题：**注意力机制的焦点固化**：注意力过度集中在最近生成的几个词上，导致后续生成的内容和近期历史最高相似；**最大似然估计训练目标**：最大似然会让模型倾向于生成*安全*、*常见*的保守序列、重复就是一种相对安全的模式； **context 长度限制**：在长文本处理中，模型无法有效感知和利用到更早的上下文，可能导致在局部信息中循环
- prompt不太行：模糊和引导行不足的prompt也会让模型无所适从，更容易陷入重复

### 怎么解决
- 调整解码策略：引入随机性，topk、topp+合适的temperature
- 重复惩罚项：解码中，对生成过的token施加重复惩罚，降低其再次被选中的概率；基于频率、基于存在性
- N-gram阻止：禁止生成与最近历史中完全相同的N-gram，比如设置 no_repeat_ngram=3，可以防止连续三个词的重复
- 改prompt：加入few-shot、提供更加清晰、具体的指令、prompt中直接要求避免重复
- 模型层面改进：1.使用高质量、多样化的数据进行微调，特别是包含长距离依赖和避免重复的样本；2.训练目标改进，使用对比学习或者强化学习的方式替代最大似然，鼓励生成更有趣、信息量更大的内容
- **后处理**：对生成内容进行检测或者修正（精准打击，但是治标不治本）

## 数据角度看问题
1. 数据偏见：训练数据来源于互联网等真实世界，不可避免地包含了各种社会偏见（如性别、种族、地域、文化偏见）。模型学习这些偏见后，会在生成内容时表现出来，导致不公平或歧视性的输出
2. 毒性和有毒的内容：数据中包含大量的不良信息，如仇恨言论、暴力内容、虚假信息、成人内容等。模型可能学习并生成这些有害内容
3. 数据质量问题：*噪声和错误*，数据中包含拼写错误、语法错误、事实性错误、格式混乱等；*不一致*，不同来源的数据可能存在相互矛盾；*冗余+重复*，大量重复和高度相似的内容影响模型学习的效率和多样性
4. 数据隐私泄漏风险：训练数据可能无意中包含个人身份信息（PII）、商业机密或其他敏感信息。模型可能在生成时“记住”并泄露这些信息。**去敏**
5. 版权和知识产权问题：训练数据中使用了大量受版权保护的材料，其合法性存在争议，可能引发法律风险。模型生成的内容也可能与现有版权作品过于相似。
6. 数据覆盖面和代表性不足：*领域和语言的不均衡*，主要以英语和常见领域为主；*知识陈旧性*，模型知识截止其训练数据收集的时间点
7. 数据标注成本和一致性：对齐数据中，需要监督学习的任务（如 SFT、RLHF），高质量的数据标注成本高昂，且标注者之间可能存在主观差异，影响数据一致性。
8. 数据投毒攻击风险：恶意攻击者可能故意向训练数据中注入错误或有害的信息，以破坏模型的性能或使其产生特定倾向的输出。
### 决策流程建议:
- 尝试 Zero-shot/Few-shot: 先用好的 Prompt 测试预训练模型在目标任务上的表现。
- 评估结果: 如果效果满足要求，则无需微调。
- 效果不佳，考虑微调: 如果效果不理想，且有可用的标注数据和资源，考虑进行微调（优先考虑 PEFT 方法）。
- 效果仍不佳或需动态知识: 可能需要结合 RAG 或更复杂的策略。

## SFT导致的通用能力遗忘应该怎么解决?

SFT (Supervised Fine-tuning) 旨在让模型学会遵循指令或在特定任务上表现更好。但只用特定任务的数据进行微调，容易导致模型“过度拟合”到该任务上，从而损害其在预训练阶段学到的广泛通用能力（如常识推理、其他类型的任务），这种现象称为灾难性遗忘 (Catastrophic Forgetting)。

解决方法:

### 数据策略 (Data Strategies):
混合训练数据 (Data Mixing / Replay): 在 SFT 阶段，不要只使用目标任务的数据，而是将其与一部分预训练数据或来自其他通用任务的指令数据混合在一起训练。这样可以在学习新任务的同时，“复习”通用能力。
高质量多样化的 SFT 数据: 确保 SFT 数据集本身足够多样，覆盖不同的指令类型和场景，避免过于狭窄导致模型能力退化。
参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT): 这是目前最常用和有效的方法之一。
原理: PEFT 方法（如 LoRA, Adapter Tuning, Prefix Tuning, Prompt Tuning等）在微调时只训练模型参数的一小部分（或增加少量新参数），而保持大部分预训练参数不变。
效果: 由于核心的预训练参数被冻结，模型能很大程度上保留其通用能力，同时新增/修改的少量参数足以适配新任务。
### 正则化方法 (Regularization Techniques):
L2 正则化: 对权重变化进行惩罚，限制模型参数偏离预训练状态太远。
其他知识蒸馏/约束方法: 例如，在微调时加入一个额外的损失项，要求模型的输出在某些通用任务上与原始预训练模型保持一致。
多任务学习 (Multi-Task Learning): 同时在多个任务（包括目标特定任务和一些通用任务）上进行微调，让模型在不同能力维度上保持平衡。
### 控制微调程度 (Controlling Fine-tuning Extent):
更少的训练轮数 (Epochs): 不要过度训练，在模型开始在新任务上表现良好但尚未严重遗忘通用能力时停止训练。
更低的学习率 (Learning Rate):* 使用较小的学***率进行微调，减小参数更新的幅度。
持续预训练 (Continued Pre-training) + 微调: 在 SFT 之前，先在与目标领域相关的通用数据上进行一轮持续预训练，然后再进行 SFT，可能有助于模型更好地适应新领域而不丢失通用性。
回答要点: 解释什么是灾难性遗忘。重点介绍 PEFT 方法作为主流解决方案。然后补充数据混合、正则化、多任务学习、控制微调程度等其他有效策略
## from js
1. 数据类
- 怎么抓数据
- 怎么提升数据鲁棒性
- 数据增强

## 位置编码：RoPE、Abili、YARN
## 预训练、SFT、RLHF（DPO vs PPO）
## 超长上下文（外推、 sparseattention、linearattention）
## 摘要生成模型怎么训练
## 预训练数据清洗、数据配比等，怎么操做可以让模型性能更好
得看技术报告啦

## 大模型“复读机”问题出现的原因，业内的解决方案
### 原因
1. 大模型通常基于互联网文本训练，而网络数据中普遍存在重复内容（如新闻标题、社交媒体高频短语）
2. 自回归的训练目标，模型进行ntp任务，可能会更倾向于选择高频词或者最近出现过的内容
3. 解码策略：使用贪心或者beam search这些的话可能会陷入局部最优，从而陷入循环
### 解决
1. topk + topP+ temperature
2. prompt引导模型，通过icl的方式跳出重复
3. 训练的时候对数据进行去重、构造更加多样的数据
4. 推理的时候可以加入一些重复的惩罚项

## beam search原理+实现
## flashattention V1、V2
## L1 和 L2正则化差别：L1（增加的权重是权重向量的绝对值之和，会让模型往参数更加稀疏、让某些参数变成0，更加适用于特征选择）L2（增加的权重是权重向量的平方和，再减少权重的同时会保持所有参数非0，权重的调整更加平稳）

## MOE为什么比dense难训练
1. 模型结构更加复杂：gate网络+专家网络
2. gate网络和专家网络的优化目标存在冲突，门控需要高效分配样本，但是专家网络是专注特定模式学习；例如门控网络可能更加倾向将样本分配给容易训练的专家
3. 初始化困难，如果初始化差别过大，导致gate过早固化路径，就会导致部分专家欠训练，推理的时候就不会激活这部分，导致浪费
4. moe更加稀疏，然后门控的存在也会导致每个批次训练训练信号仅作用于部分专家，导致整体收敛速度慢；同时，moe稀疏的参数训练的显存要求、通信要求都更加多

## sigmoid为什么会导致梯度消失
## KL散度和交叉熵的差别
## encoder怎么降R


### 1.强化学习前为什么要使用冷启动数据（SFT）
1. 主要是方式ref模型采样到的生成文本超出了奖励模型的判断范围，经过冷启动数据微调之后可以保障是ref模型的输出和采样结果在奖励模型所训练的样本分布范围之内
比如 r1-zero，没有经过冷启动，就RL之后出现的了一些生成文本可读性差、语言使用混乱的问题

### 2.强化学习的必要性
1. 对齐税的存在，sft泛化能力上限是比较差的；sft的数据大多是是人工构造的，成本会比较高
2. 强化学习可以采样构造出无限的样本，通过模型采样来构造数据，奖励模型来判断好坏，会有更强的泛化能力。（但是对齐的模型和奖励模型要同步进步，不然奖励模型会出现ood问题）

## 3. PPO、DPO
### PPO
训练中需要梯度反向传播：policy model + value model
为什么有了奖励模型还需要value model：1.reward model在PPO训练中是不参与梯度反向传播的，reward model 的奖励来自人类标注的偏好数据，意味着对每一个时刻来说，reward model能给出一个更加客观的评价。2.根据贝尔曼方程，RL中需要一个对句子未来收益的预估
pg_loss 和 value_loss：pg_loss 是policy model的损失，ratio * advantage（重要性采样*优势）；value_loss 估计训练过程中的预估收益，MSE；为了训练的稳定性，加入clip来防止异常值

reward model怎么训：reward model的功能是代替人类来对模型的动作进行打分，所以reward model的评估方式是考查reward model反馈的reward和人类真实偏好之间的接近程度。
防止 reward hacking（不符合人类的真实偏好+reward 异常高）：policy模型找到一个trick的方法来获取高分，所以业内一般reward model 和 policy model的尺寸相当，泛化能力比较接近，防止reward model 被policy model欺骗；2. 偏好数据的数量和多样性，有利于让reward model接近真实的人类偏好，也能防止过拟合

怎么稳定PPO训练：1.合理的监控（reward、 困惑度（损失）、KL散度、生成文本长度）；损失和梯度进行裁剪平；2.对reward 或者优势分数进行裁剪,但是优势分数裁剪比较难，reward比较方便）；对策略更新的幅度进行裁剪；对梯度进行裁剪；3.加入词元级别的KL散度、加入预训练的损失

### DPO
主要解决什么问题，理论+实现
如何避免依赖复杂的强化学习框架，直接使用偏好数据集+优化策略实现大模型的偏好对齐
1. DPO是基于BT模型进行推演的，利用从奖励函数到最有策略的解析映射，来对奖励函数的损失转成对策略的损失

### PPO vs DPO
1. 计算资源：DPO是PPO的一半，
2. 训练稳定性上：DPO高于PPO，DPO只有一个超参数 beta
3. 效果 PPO的泛化强于DPO：1.DPO是让策略模型对从偏好数据进行策略优化，但是偏好数据难以刻画真实的偏好分布；2. PPO使用在线学习的方式，最大化任意提示生成回复时的奖励，然后生成回复中的随机解码也会带来一定的探索空间；
如何优化DPO：先用偏好数据的正例来微调模型，再进行DPO。相当于冷启动的时候让模型偏好是正例


## SFT和RLHF有什么不同
1. SFT是有监督是学习人类偏好的指令，让模型更加适应人类交互的风格；SFT只有“好”的数据
2. RLHF通过强化学习让模型输出更加符合人类价值观预期；RLHF训练依赖“好”和“差”的对比，可以学习负反馈


## DP TP SP PP EP

## deepspeed