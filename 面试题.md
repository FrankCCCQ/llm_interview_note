todo：投机采样、medusa heads、continuous batching


## 1. 大模型推理优化
归纳为6类：分布式优化、低比特优化、算子优化、访存优化、服务并发优化、其他
分布式优化：通信层面的优化（TP、DP、PP + nccl通信优化）
低比特优化：降低显存占用（int8、int4权重量化｜AWQ自适应量化、KV cache）
算子优化：提高cuda算子的计算效率（算子融合和GEMM高斯该性能算子）
访存优化：减少gpu对hbm的读写（flashattention）
服务并发优化：continues batching、dynamic batching、async serveing
其他：投机采样、medusa heads、lookahead decoding、eagle

## 2. 大模型的吞吐太小怎么办？
吞吐率 = 处理请求数量 / 处理时间

处理请求数量： batch_size，node数量
处理时间 ： 模型forward的时间

### 1. 模型单次推理的时间 + 模型能支持的并发数量 -> 降低模型的推理延时+提供并发数量
如何降低推理延时：权重+激活量化，比如GPU利用率有空余，可以采用投机采样（小模型猜测+大模型验证）；访存优化：flashattention
如何提高模型的并发数：1. batch_size增加（静态batch、 动态batch、 continuous batching）；业务允许的情况下做水平扩展，增加资源数量，负载均衡

### 2. 结合业务
1. 降低推理延时：优化单个请求的体验；提高并行度，优化的多个请求的处理时间，但对每个请求可能会导致10%-30%左右的延时增加
continuous batching：LLM场景下不能假定请求的输入序列和输出序列的长度，所以对于静态batching和动态batching来说，输入输出的变化可能会严重GPU的利用率降低，所以使用**continuous batching迭代调度**（当一个序列出现一个结束生成的标记，就可以在这个序列后面插入新的序列）
服务上也需要根据是不是流式输出来进行优化。结合异步进行解码


## 如何解决badcase
1. 前置模块 + 后处理 + 调整prompt + 模型微调
*前置模块*：对于一些高频case，直接在前置模块进行返回，配置一些简单规则实现
*后处理*：比如LLM幻觉，可以后处理模块在进行2次过滤，
*修改prompt*：不要求时效性前提下，比如对输出的关键词有要求、对人设遵循能力、一致性等
*模型微调*：成本比较高、微调后对原来效果可能会出现影响。所以一般累计case，固定周期进行模型更新


## DeepSeek MoE的创新与改进

### Mixtral的MoE结构（对比基线）
- 将Attention层后的MLP模块替换为MoE模块（Router + 多个Dense Expert）
- 使用8个7B参数量的Dense Expert，每次仅激活2个
- 工作流程：Token经Router计算→选择Top-k Expert激活→加权组合结果

### DeepSeek的MoE结构创新
- **组成部分**：
  - Router模块：负责分发Token到不同Expert
  - Routed Experts：256个专家，每次激活8个
  - Shared Expert：1个通用专家，始终激活
- **实现细节**：
  - Routed Experts分组管理，先计算每组内Top-2专家的相似度得分和
  - 选择得分最高的Top-k组，再从每组中选择Top-k专家
  - 最终输出 = Shared Expert输出 + Routed Experts加权输出

### DeepSeek MoE结构的优势
1. **知识分离与优化**
   - 解决传统MoE中Expert间存在的冗余问题
   - Shared Expert处理通用知识，Routed Experts专注差异化知识
   - 减轻了单个Routed Expert需要学习的知识量

2. **计算效率提升**
   - 背景：MoE训练中Expert分布在不同设备上，产生大量通信开销
   - 优化方法：在通信流处于工作状态时，同步使用计算流进行后续计算
   - 效果：Shared Expert和Routed Experts的通信相互独立，可以隐藏通信时间

  




## from js
1. 数据类
- 怎么抓数据
- 怎么提升数据鲁棒性
- 数据增强
